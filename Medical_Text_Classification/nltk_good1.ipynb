{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Huynh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Huynh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Huynh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.preprocessing import normalize\n",
    "import random\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "import random\n",
    "\n",
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14438"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('train.dat', 'r', encoding='utf8') as fh:\n",
    "    lines = fh.readlines()\n",
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('test.dat', 'r', encoding='utf8') as fh:\n",
    "    lines_t = fh.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28880\n",
      "14438\n",
      "3163 1494 1925 3051 4805\n"
     ]
    }
   ],
   "source": [
    "docs = [l[2:] for l in lines] + [l for l in lines_t]\n",
    "y = [l[0] for l in lines]\n",
    "print(len(docs))\n",
    "print(len(y))\n",
    "  \n",
    "\n",
    "regTok = RegexpTokenizer(r'[a-zA-Z]+')\n",
    "# regTok = RegexpTokenizer(r'\\w+')\n",
    "docss = []\n",
    "for i in docs:\n",
    "    i = i.lower()\n",
    "    docss.append(regTok.tokenize(i))\n",
    "\n",
    "\n",
    "s_wrds = set(stopwords.words('english'))\n",
    "lmtzr = WordNetLemmatizer()\n",
    "stmm = PorterStemmer()\n",
    "docsss = []\n",
    "for i in docss:\n",
    "    temp = [lmtzr.lemmatize(w) for w in i if not w in s_wrds]\n",
    "#     temp = [stmm.stem(w) for w in i if not w in s_wrds]\n",
    "    docsss.append(temp)\n",
    "\n",
    "docs = []\n",
    "for i in docsss:\n",
    "    temp = ''\n",
    "    for j in i:\n",
    "        temp = temp + j + ' '\n",
    "    docs.append(temp[0:-1])\n",
    "\n",
    "docss = []\n",
    "docsss = []\n",
    "\n",
    "\n",
    "c1=0\n",
    "c2=0\n",
    "c3=0\n",
    "c4=0\n",
    "c5=0\n",
    "for i in y:\n",
    "    if int(i) == 1: c1+=1\n",
    "    elif int(i) == 2: c2+=1\n",
    "    elif int(i) == 3: c3+=1\n",
    "    elif int(i) == 4: c4+=1\n",
    "    elif int(i) == 5: c5+=1\n",
    "print(c1,c2,c3,c4,c5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ryan_preprocess(data, ngram=3, max_df=0.6, min_df=1, stop_words = 'english'):\n",
    "    \n",
    "    vect = TfidfVectorizer(max_features=50000, stop_words=stop_words, max_df=max_df, min_df=min_df, ngram_range=(1, ngram), norm='l2', token_pattern='\\w{4,}')\n",
    "    vect.fit(data)\n",
    "    print(len(vect.get_feature_names()))\n",
    "#     print(vect.get_feature_names())\n",
    "    \n",
    "    csr_mat = vect.transform(data[0:14438])\n",
    "    csr_mat1 = vect.transform(data[14438:])\n",
    "    \n",
    "#     csr_mat = vect.transform(data[0:17857])\n",
    "#     csr_mat1 = vect.transform(data[17857:])\n",
    "    print(type(csr_mat))\n",
    "    \n",
    "    return csr_mat, csr_mat1\n",
    "    \n",
    "def classifyNames(mat,cls, k=3, d=10):\n",
    "    r\"\"\" Classify names using c-mer frequency vector representations of the names and kNN classification with \n",
    "    cosine similarity and 10-fold cross validation\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    def classify(x, train, clstr):\n",
    "        r\"\"\" Classify vector x using kNN and majority vote rule given training data and associated classes\n",
    "        \"\"\"\n",
    "        # find nearest neighbors for x\n",
    "\n",
    "        dots = x.dot(train.T)\n",
    "\n",
    "        sims = list(zip(dots.indices, dots.data))\n",
    "        if len(sims) == 0:\n",
    "            # could not find any neighbors\n",
    "            return '+' if np.random.rand() > 0.5 else '-'\n",
    "        sims.sort(key=lambda x: x[1], reverse=True)\n",
    "        tc = Counter(clstr[s[0]] for s in sims[:k]).most_common(5)\n",
    "# weighting the low count classes      \n",
    "#         for i in range(len(tc)):\n",
    "#             if tc[i][0] == '2': \n",
    "#                 tc[i] =  (tc[i][0], int(tc[i][1] * 1.4))\n",
    "#             elif tc[i][0] == '3': \n",
    "#                 tc[i] =  (tc[i][0], int(tc[i][1] * 1.2))\n",
    "                \n",
    "        if len(tc) < 2 or tc[0][1] > tc[1][1]:\n",
    "            # majority vote\n",
    "            return tc[0][0]\n",
    "        # tie break\n",
    "        tc = defaultdict(float)\n",
    "        for s in sims[:k]:\n",
    "            tc[clstr[s[0]]] += s[1]\n",
    "        return sorted(tc.items(), key=lambda x: x[1], reverse=True)[0][0]\n",
    "        \n",
    "    macc = 0.0\n",
    "    mf1 = 0.0\n",
    "    \n",
    "    kfold = KFold(d)\n",
    "    for train, test in kfold.split(mat):\n",
    "        clstr = []\n",
    "        clste = []\n",
    "        \n",
    "        for i in train:\n",
    "            clstr.append(y[i])    \n",
    "        for i in test:\n",
    "            clste.append(y[i])\n",
    "        \n",
    "        train = mat[train]\n",
    "        test = mat[test]\n",
    "\n",
    "        # predict the class of each test sample\n",
    "        clspr = [ classify(test[i,:], train, clstr) for i in range(test.shape[0]) ]\n",
    "        \n",
    "        acc = accuracy_score(clste, clspr)\n",
    "        print('acc: ', acc)\n",
    "        macc += acc\n",
    "        \n",
    "        f1 = f1_score(clste, clspr, average='weighted')\n",
    "        print('f1: ', f1)\n",
    "        mf1 += f1\n",
    "        \n",
    "    return macc/d, mf1/d\n",
    "\n",
    "\n",
    "def get_pred(mat,mat1,cls, k=3, d=10):\n",
    "    r\"\"\" Classify names using c-mer frequency vector representations of the names and kNN classification with \n",
    "    cosine similarity and 10-fold cross validation\n",
    "    \"\"\"\n",
    "    \n",
    "    def classify(x, train, clstr):\n",
    "        r\"\"\" Classify vector x using kNN and majority vote rule given training data and associated classes\n",
    "        \"\"\"\n",
    "        # find nearest neighbors for x\n",
    "\n",
    "        dots = x.dot(train.T)\n",
    "\n",
    "        sims = list(zip(dots.indices, dots.data))\n",
    "        if len(sims) == 0:\n",
    "            # could not find any neighbors\n",
    "            return '+' if np.random.rand() > 0.5 else '-'\n",
    "        sims.sort(key=lambda x: x[1], reverse=True)\n",
    "        tc = Counter(clstr[s[0]] for s in sims[:k]).most_common(5)\n",
    "        if len(tc) < 2 or tc[0][1] > tc[1][1]:\n",
    "            # majority vote\n",
    "            return tc[0][0]\n",
    "        # tie break\n",
    "        tc = defaultdict(float)\n",
    "        for s in sims[:k]:\n",
    "            tc[clstr[s[0]]] += s[1]\n",
    "        return sorted(tc.items(), key=lambda x: x[1], reverse=True)[0][0]\n",
    "    \n",
    "    # predict the class of each test sample\n",
    "    clspr = [ classify(mat1[i,:], mat, cls) for i in range(mat1.shape[0]) ]\n",
    "    return clspr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "acc:  0.6094182825484764\n",
      "f1:  0.6022179453396013\n",
      "acc:  0.6121883656509696\n",
      "f1:  0.6049132590842182\n",
      "acc:  0.6378116343490304\n",
      "f1:  0.63170407799652\n",
      "acc:  0.6253462603878116\n",
      "f1:  0.6172254907576681\n",
      "acc:  0.6135734072022161\n",
      "f1:  0.6044820187750563\n",
      "acc:  0.6142659279778393\n",
      "f1:  0.606179492750909\n",
      "acc:  0.6135734072022161\n",
      "f1:  0.6076130474409603\n",
      "acc:  0.6087257617728532\n",
      "f1:  0.6002989682050679\n",
      "acc:  0.6292446292446292\n",
      "f1:  0.6232704824211004\n",
      "acc:  0.6008316008316008\n",
      "f1:  0.5890381904671824\n",
      "average acc:  0.6164979277167643\n",
      "average f1:  0.6086942973238283\n",
      "================\n"
     ]
    }
   ],
   "source": [
    "csr_mat, csr_mat1 = ryan_preprocess(docs, max_df=0.7, ngram=6, min_df=1, stop_words='english')\n",
    "\n",
    "k = 40\n",
    "acc, f1_score = classifyNames(csr_mat,y, k = k, d = 10)\n",
    "print(\"average acc: \", acc)\n",
    "print(\"average f1: \", f1_score)\n",
    "print('================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 40\n",
    "csr_mat, csr_mat1 = ryan_preprocess(docs, max_df=0.7, ngram=6, min_df=1, stop_words='english')\n",
    "\n",
    "pred = get_pred(csr_mat,csr_mat1, y, k = k)\n",
    "\n",
    "\n",
    "with open(\"result_good.dat\", \"w\") as f: \n",
    "    for i in pred:\n",
    "        f.write(i)\n",
    "        f.write('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
