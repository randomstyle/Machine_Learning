{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import h5py\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolve window\n",
    "# Create mask\n",
    "# -------------\n",
    "\n",
    "\n",
    "# Pooling forward (max, average)\n",
    "# Pooling backward (max, average)\n",
    "# Convolution forward (for all data)\n",
    "# Convolution backward (for all data)\n",
    "\n",
    "# Forward propagation (including all steps)\n",
    "# Backward propagation (including all steps)\n",
    "# Parameter updating (Gradient descent or other optimization method)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ryan_cal_Z(W,b,A):\n",
    "    Z = np.dot(W, A) + b\n",
    "    cache = [A,W,b,Z]\n",
    "    return Z, cache\n",
    "\n",
    "def ryan_initweight_convol(fil_size):\n",
    "    np.random.seed(1)\n",
    "    params ={}\n",
    "    temp = 2/math.sqrt(1020+6)\n",
    "    for i in range(1, len(fil_size)+1):\n",
    "        params['W' + str(i)] = temp*np.random.randn(fil_size[i-1][0],fil_size[i-1][1],fil_size[i-1][2],fil_size[i-1][3])\n",
    "        params['b' + str(i)] = temp*np.random.randn(1,1,1,fil_size[i-1][3])\n",
    "        \n",
    "    return params\n",
    "        \n",
    "def ryan_initweight(topo, params):\n",
    "    np.random.seed(1)\n",
    "    temp = int(len(params)//2)\n",
    "    for i in range(1, len(topo)):\n",
    "        params['W' + str(i+temp)] = 0.01*np.random.randn(topo[i], topo[i-1])\n",
    "        params['b' + str(i+temp)] = 0.01*np.zeros(shape=(topo[i], 1))\n",
    "    return params\n",
    "\n",
    "def ryan_save_model(params, path):\n",
    "    f = h5py.File(path,'w')\n",
    "    for key, value in params.items():\n",
    "        f.create_dataset(key, data=value)\n",
    "    f.close()\n",
    "    return f\n",
    "\n",
    "def ryan_load_model(path):\n",
    "    dataset = h5py.File(path,'r')\n",
    "    params = {}\n",
    "    for i in dataset.keys():\n",
    "        params[i] = np.array(dataset[i])\n",
    "    \n",
    "    dataset.close()\n",
    "    return params\n",
    "        \n",
    "def ryan_onehot(labels, df=1):\n",
    "    if df == 1:\n",
    "        labels = labels.as_matrix()\n",
    "    temp = []\n",
    "    for val in labels:\n",
    "        if val not in temp:\n",
    "            temp.append(val)\n",
    "    temp.sort()\n",
    "    \n",
    "    result = np.zeros(shape=(len(labels),len(temp)))\n",
    "    for key, val in enumerate(labels):\n",
    "        result[key][temp.index(val)] = 1\n",
    "    return result\n",
    "\n",
    "def ryan_padding(X, pads):\n",
    "    result = np.pad(X,((pads[0],pads[0]),(pads[1],pads[1]),(pads[2],pads[2]),(pads[3],pads[3])),'constant');\n",
    "    return result\n",
    "\n",
    "def ryan_convol_step(one_slice, W, b):\n",
    "    Z = np.multiply(one_slice, W) + b\n",
    "    Z = np.sum(Z)\n",
    "    return Z\n",
    "    \n",
    "def ryan_compute_cost(lastA, Y):\n",
    "    t1 = np.multiply(Y, np.log(lastA))\n",
    "    t2 = np.multiply(1.0 - Y, np.log(1.0 - lastA))\n",
    "    cost = (-1.0/Y.shape[1]) * np.sum(t1 + t2)\n",
    "    return cost\n",
    "\n",
    "# activation functions\n",
    "def ryan_relu(Z):\n",
    "    A = np.maximum(0, Z)\n",
    "    return A\n",
    "\n",
    "def ryan_relu_back(dA, Z):\n",
    "    t = Z >= 0\n",
    "    dZ = np.multiply(dA, t)\n",
    "    return dZ\n",
    "\n",
    "def ryan_sigmoid(Z):\n",
    "    A = 1.0/(1 + np.exp(-1.0*(Z+1e-7)))\n",
    "    return A\n",
    "\n",
    "def ryan_sigmoid_back(dA, Z):\n",
    "    temp = 1.0/(1 + np.exp(-1*Z))\n",
    "    dZ = np.multiply(dA, np.multiply(temp, (np.subtract(np.ones(temp.shape), temp))))\n",
    "    return dZ\n",
    "\n",
    "def ryan_init_Z(A, W, s, p):\n",
    "\n",
    "    (m, dim, _, _) = A.shape\n",
    "    (f, _, _, c) = W.shape\n",
    "    \n",
    "    dim = int((dim+2*p-f)/s)+1\n",
    "    \n",
    "    Z = np.zeros((m, dim, dim, c))\n",
    "    A1 = np.zeros((m, dim, dim, c))\n",
    "    return Z, A1, dim, f\n",
    "    \n",
    "def ryan_convol_forward(A, W, b, s, p):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for a convolution function\n",
    "    \n",
    "    Arguments:\n",
    "    A_prev -- output activations of the previous layer, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    W -- Weights, numpy array of shape (f, f, n_C_prev, n_C)\n",
    "    b -- Biases, numpy array of shape (1, 1, 1, n_C)\n",
    "    hparameters -- python dictionary containing \"stride\" and \"pad\"\n",
    "        \n",
    "    Returns:\n",
    "    Z -- conv output, numpy array of shape (m, n_H, n_W, n_C)\n",
    "    cache -- cache of values needed for the conv_backward() function\n",
    "    \"\"\"\n",
    "    \n",
    "    Z, A1, dim, f = ryan_init_Z(A, W, s, p)\n",
    "    A_pad = ryan_padding(A, [0,p,p,0])\n",
    "    for i in range(Z.shape[0]):                                 # loop over the batch of training examples\n",
    "        a_prev_pad = A_pad[i]                     # Select ith training example's padded activation\n",
    "        for h in range(dim):                           # loop over vertical axis of the output volume\n",
    "            for w in range(dim):                       # loop over horizontal axis of the output volume\n",
    "                for c in range(Z.shape[3]):                   # loop over channels (= #filters) of the output volume\n",
    "                    # Find the corners of the current \"slice\" (≈4 lines)\n",
    "                    vert_start = h * s\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w * s\n",
    "                    horiz_end = horiz_start + f\n",
    "                    # Use the corners to define the (3D) slice of a_prev_pad (See Hint above the cell). (≈1 line)\n",
    "                    a_slice_prev = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]\n",
    "                    # Convolve the (3D) slice with the correct filter W and bias b, to get back one output neuron. (≈1 line)\n",
    "                    Z[i, h, w, c] = ryan_convol_step(a_slice_prev, W[..., c], b[..., c])\n",
    "                    A1[i, h, w, c] = ryan_relu(Z[i, h, w, c])\n",
    "                    \n",
    "    cache = [A, W, b, s, p, Z]\n",
    "    \n",
    "    return A1, cache\n",
    "\n",
    "def ryan_pool_forward(A, s, f, mode = \"max\"):\n",
    "    \"\"\"\n",
    "    Implements the forward pass of the pooling layer\n",
    "    \n",
    "    Arguments:\n",
    "    A_prev -- Input data, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    hparameters -- python dictionary containing \"f\" and \"stride\"\n",
    "    mode -- the pooling mode you would like to use, defined as a string (\"max\" or \"average\")\n",
    "    \n",
    "    Returns:\n",
    "    A -- output of the pool layer, a numpy array of shape (m, n_H, n_W, n_C)\n",
    "    cache -- cache used in the backward pass of the pooling layer, contains the input and hparameters \n",
    "    \"\"\"\n",
    "    \n",
    "  \n",
    "#     (m, n_H_prev, n_W_prev, n_C_prev) = A.shape\n",
    "    # Define the dimensions of the output\n",
    "    dim = int(1 + (A.shape[1] - f) / s)\n",
    "    \n",
    "    \n",
    "    # Initialize output matrix A\n",
    "    A_pool = np.zeros((A.shape[0], dim, dim, A.shape[3]))\n",
    "\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    for i in range(A.shape[0]):                           # loop over the training examples\n",
    "        for h in range(dim):                     # loop on the vertical axis of the output volume\n",
    "            for w in range(dim):                 # loop on the horizontal axis of the output volume\n",
    "                for c in range (A.shape[3]):            # loop over the channels of the output volume\n",
    "                    \n",
    "                    # Find the corners of the current \"slice\" (≈4 lines)\n",
    "                    vert_start = h * s\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w * s\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    # Use the corners to define the current slice on the ith training example of A_prev, channel c. (≈1 line)\n",
    "                    a_prev_slice = A[i, vert_start:vert_end, horiz_start:horiz_end, c]\n",
    "                    \n",
    "                    # Compute the pooling operation on the slice. Use an if statment to differentiate the modes. Use np.max/np.mean.\n",
    "                    if mode == \"max\":\n",
    "                        A_pool[i, h, w, c] = np.max(a_prev_slice)\n",
    "                    elif mode == \"average\":\n",
    "                        A_pool[i, h, w, c] = np.mean(a_prev_slice)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Store the input and hparameters in \"cache\" for pool_backward()\n",
    "    cache = (A, f, s)\n",
    "        \n",
    "    return A_pool, cache\n",
    "\n",
    "def ryan_forward_all(X, params):\n",
    "    A=X\n",
    "    con_caches={}\n",
    "    full_caches={}\n",
    "    \n",
    "    #l1\n",
    "    A,cache=ryan_convol_forward(A,params['W' + str(1)],params['b' + str(1)], 2,1)\n",
    "    A,cache1 = ryan_pool_forward(A, 1, 5, 'max')\n",
    "    con_caches['con1'] = cache\n",
    "    con_caches['pool1'] = cache1\n",
    "    \n",
    "    #l2\n",
    "    A,cache=ryan_convol_forward(A,params['W' + str(2)],params['b' + str(2)], 2,0)\n",
    "    A,cache1 = ryan_pool_forward(A, 1, 5, 'average')\n",
    "    con_caches['con2'] = cache\n",
    "    con_caches['pool2'] = cache1\n",
    "    \n",
    "    #flatten\n",
    "    result = np.zeros((1020,1296))\n",
    "    for i in range(A.shape[0]):\n",
    "        result[i] = A[i].flatten()\n",
    "    result = result.T\n",
    "    \n",
    "    #l3\n",
    "    Z, cache = ryan_cal_Z(params['W' + str(3)], params['b' + str(3)], result)\n",
    "    full_caches['full1'] = cache\n",
    "    result = ryan_relu(Z)\n",
    "\n",
    "    #l4\n",
    "    Z, cache = ryan_cal_Z(params['W' + str(4)], params['b' + str(4)], result)\n",
    "    full_caches['full2'] = cache\n",
    "    result = ryan_sigmoid(Z)\n",
    "    \n",
    "    return result, con_caches, full_caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.load('ex5_train_x.npy')\n",
    "X_train = X_train/255 - 0.5\n",
    "Y_train = np.load('ex5_train_y.npy')\n",
    "\n",
    "Y_onehot = ryan_onehot(Y_train, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1020, 64, 64, 3)\n",
      "(1020, 6)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(Y_onehot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = ryan_initweight_convol(fil_size=[(4,4,3,8),(4,4,8,16)])\n",
    "params = ryan_initweight([1296,108,6],params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 4, 3, 8)\n",
      "(1, 1, 1, 8)\n",
      "--------------\n",
      "(4, 4, 8, 16)\n",
      "(1, 1, 1, 16)\n",
      "---------------\n",
      "(108, 1296)\n",
      "(108, 1)\n",
      "--------------\n",
      "(6, 108)\n",
      "(6, 1)\n"
     ]
    }
   ],
   "source": [
    "print(params['W1'].shape)\n",
    "print(params['b1'].shape)\n",
    "print(\"--------------\")\n",
    "print(params['W2'].shape)\n",
    "print(params['b2'].shape)\n",
    "print(\"---------------\")\n",
    "print(params['W3'].shape)\n",
    "print(params['b3'].shape)\n",
    "print(\"--------------\")\n",
    "print(params['W4'].shape)\n",
    "print(params['b4'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ryan_update_params(params, grads, learning_rate):\n",
    "    for i in range(int(len(params)//2)):\n",
    "        params[\"W\" + str(i + 1)] = params[\"W\" + str(i + 1)] - learning_rate * grads[\"dW\" + str(i + 1)]\n",
    "        params[\"b\" + str(i + 1)] = params[\"b\" + str(i + 1)] - learning_rate * grads[\"db\" + str(i + 1)]\n",
    "        \n",
    "    return params\n",
    "\n",
    "def ryan_cal_grads(dZ, cache):\n",
    "    m = cache[0].shape[1]\n",
    "    dW = np.dot(dZ, cache[0].T)/m\n",
    "    db = np.sum(dZ, axis=1, keepdims=True)/m\n",
    "    dA = np.dot(cache[1].T, dZ)\n",
    "    \n",
    "    return dA, dW, db\n",
    "\n",
    "def ryan_backward_all(lastA, Y, full_caches, con_caches):\n",
    "    m = Y.shape[1]\n",
    "    grads = {}\n",
    "    temp1, temp2, temp3 = 0, 0, 0\n",
    "    temp1 = np.divide(1 - Y, 1 - lastA) - np.divide(Y, lastA)\n",
    "    \n",
    "    #l4\n",
    "    current_cache = full_caches['full2']\n",
    "    dZ = ryan_sigmoid_back(temp1, current_cache[-1])\n",
    "    temp1, temp2, temp3 = ryan_cal_grads(dZ, current_cache[0:-1])\n",
    "    grads[\"dA4\"] = temp1\n",
    "    grads[\"dW4\"] = temp2\n",
    "    grads[\"db4\"] = temp3\n",
    "\n",
    "    #l3\n",
    "    current_cache = full_caches['full1']\n",
    "    dZ = ryan_relu_back(temp1, current_cache[-1])\n",
    "    temp1, temp2, temp3 = ryan_cal_grads(dZ, current_cache[0:-1])\n",
    "    grads[\"dA3\"] = temp1\n",
    "    grads[\"dW3\"] = temp2\n",
    "    grads[\"db3\"] = temp3\n",
    "    \n",
    "    #l2\n",
    "    temp1 = temp1.T.reshape((1020,9,9,16))\n",
    "    dA_prev = ryan_pool_backward(temp1, con_caches['pool2'], 'average')\n",
    "    dZ = ryan_relu_back(dA_prev, con_caches['con2'][-1])\n",
    "    dA,dW,db = ryan_conv_backward(dZ, con_caches['con2'])\n",
    "    grads[\"dA2\"] = dA\n",
    "    grads[\"dW2\"] = dW\n",
    "    grads[\"db2\"] = db\n",
    "\n",
    "    #l1\n",
    "    print(\"-----------------------------\")\n",
    "    dA_prev = ryan_pool_backward(dA, con_caches['pool1'], 'max')\n",
    "    dZ = ryan_relu_back(dA_prev, con_caches['con1'][-1])\n",
    "    dA,dW,db = ryan_conv_backward(dZ, con_caches['con1'])\n",
    "    grads[\"dA1\"] = dA\n",
    "    grads[\"dW1\"] = dW\n",
    "    grads[\"db1\"] = db\n",
    "    \n",
    "    return grads\n",
    "\n",
    "def ryan_conv_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a convolution function\n",
    "    \n",
    "    Arguments:\n",
    "    dZ -- gradient of the cost with respect to the output of the conv layer (Z), numpy array of shape (m, n_H, n_W, n_C)\n",
    "    cache -- cache of values needed for the conv_backward(), output of conv_forward()\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- gradient of the cost with respect to the input of the conv layer (A_prev),\n",
    "               numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    dW -- gradient of the cost with respect to the weights of the conv layer (W)\n",
    "          numpy array of shape (f, f, n_C_prev, n_C)\n",
    "    db -- gradient of the cost with respect to the biases of the conv layer (b)\n",
    "          numpy array of shape (1, 1, 1, n_C)\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    # Retrieve information from \"cache\"\n",
    "    (A_prev, W, b, s, p, _) = cache\n",
    "    \n",
    "    # Retrieve dimensions from A_prev's shape\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    # Retrieve dimensions from W's shape\n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "    \n",
    "    # Retrieve dimensions from dZ's shape\n",
    "    (m, n_H, n_W, n_C) = dZ.shape\n",
    "    \n",
    "    # Initialize dA_prev, dW, db with the correct shapes\n",
    "    dA_prev = np.zeros((m, n_H_prev, n_W_prev, n_C_prev))                           \n",
    "    dW = np.zeros((f, f, n_C_prev, n_C))\n",
    "    db = np.zeros((1, 1, 1, n_C))\n",
    "\n",
    "    # Pad A_prev and dA_prev\n",
    "    A_prev_pad = ryan_padding(A_prev, [0,p,p,0])\n",
    "    dA_prev_pad = ryan_padding(dA_prev, [0,p,p,0])\n",
    "\n",
    "    \n",
    "    for i in range(m):                       # loop over the training examples\n",
    "        \n",
    "        # select ith training example from A_prev_pad and dA_prev_pad\n",
    "        a_prev_pad = A_prev_pad[i]\n",
    "        da_prev_pad = dA_prev_pad[i]\n",
    "        \n",
    "        for h in range(n_H):                   # loop over vertical axis of the output volume\n",
    "            for w in range(n_W):               # loop over horizontal axis of the output volume\n",
    "                for c in range(n_C):           # loop over the channels of the output volume\n",
    "                    \n",
    "                    # Find the corners of the current \"slice\"\n",
    "                    vert_start = h\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    # Use the corners to define the slice from a_prev_pad\n",
    "                    a_slice = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]\n",
    "\n",
    "                    # Update gradients for the window and the filter's parameters using the code formulas given above\n",
    "                    da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[i, h, w, c]\n",
    "                    dW[:,:,:,c] += a_slice * dZ[i, h, w, c]\n",
    "                    db[:,:,:,c] += dZ[i, h, w, c]\n",
    "                    \n",
    "\n",
    "\n",
    "        # Set the ith training example's dA_prev to the unpaded da_prev_pad (Hint: use X[pad:-pad, pad:-pad, :])\n",
    "#         print(da_prev_pad.shape)\n",
    "        if(p==0):\n",
    "            dA_prev[i, :, :, :] = da_prev_pad\n",
    "        else:\n",
    "            dA_prev[i, :, :, :] = da_prev_pad[p:-p, p:-p, :]\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask_from_window(x):\n",
    "    \"\"\"\n",
    "    Creates a mask from an input matrix x, to identify the max entry of x.\n",
    "    \n",
    "    Arguments:\n",
    "    x -- Array of shape (f, f)\n",
    "    \n",
    "    Returns:\n",
    "    mask -- Array of the same shape as window, contains a True at the position corresponding to the max entry of x.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈1 line)\n",
    "    mask = x == np.max(x)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return mask\n",
    "\n",
    "def distribute_value(dz, shape):\n",
    "    \"\"\"\n",
    "    Distributes the input value in the matrix of dimension shape\n",
    "    \n",
    "    Arguments:\n",
    "    dz -- input scalar\n",
    "    shape -- the shape (n_H, n_W) of the output matrix for which we want to distribute the value of dz\n",
    "    \n",
    "    Returns:\n",
    "    a -- Array of size (n_H, n_W) for which we distributed the value of dz\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Retrieve dimensions from shape (≈1 line)\n",
    "    (n_H, n_W) = shape\n",
    "    \n",
    "    # Compute the value to distribute on the matrix (≈1 line)\n",
    "    average = dz / (n_H * n_W)\n",
    "    \n",
    "    # Create a matrix where every entry is the \"average\" value (≈1 line)\n",
    "    a = np.ones(shape) * average\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return a\n",
    "\n",
    "def ryan_pool_backward(dA, cache, mode = \"max\"):\n",
    "    \"\"\"\n",
    "    Implements the backward pass of the pooling layer\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- gradient of cost with respect to the output of the pooling layer, same shape as A\n",
    "    cache -- cache output from the forward pass of the pooling layer, contains the layer's input and hparameters \n",
    "    mode -- the pooling mode you would like to use, defined as a string (\"max\" or \"average\")\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- gradient of cost with respect to the input of the pooling layer, same shape as A_prev\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Retrieve information from cache (≈1 line)\n",
    "    (A_prev, f, s) = cache\n",
    "    \n",
    "    # Retrieve dimensions from A_prev's shape and dA's shape (≈2 lines)\n",
    "    m, n_H_prev, n_W_prev, n_C_prev = A_prev.shape\n",
    "    m, n_H, n_W, n_C = dA.shape\n",
    "    \n",
    "    # Initialize dA_prev with zeros (≈1 line)\n",
    "    dA_prev = np.zeros(A_prev.shape)\n",
    "    \n",
    "    for i in range(m):                       # loop over the training examples\n",
    "        # select training example from A_prev (≈1 line)\n",
    "        a_prev = A_prev[i]\n",
    "        for h in range(n_H):                   # loop on the vertical axis\n",
    "            for w in range(n_W):               # loop on the horizontal axis\n",
    "                for c in range(n_C):           # loop over the channels (depth)\n",
    "                    # Find the corners of the current \"slice\" (≈4 lines)\n",
    "                    vert_start = h\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    # Compute the backward propagation in both modes.\n",
    "                    if mode == \"max\":\n",
    "                        # Use the corners and \"c\" to define the current slice from a_prev (≈1 line)\n",
    "                        a_prev_slice = a_prev[vert_start:vert_end, horiz_start:horiz_end, c]\n",
    "                        # Create the mask from a_prev_slice (≈1 line)\n",
    "                        mask = create_mask_from_window(a_prev_slice)\n",
    "                        # Set dA_prev to be dA_prev + (the mask multiplied by the correct entry of dA) (≈1 line)\n",
    "                        dA_prev[i, vert_start:vert_end, horiz_start:horiz_end, c] += np.multiply(mask, dA[i, h, w, c])\n",
    "                        \n",
    "                    elif mode == \"average\":\n",
    "                        # Get the value a from dA (≈1 line)\n",
    "                        da = dA[i, h, w, c]\n",
    "                        # Define the shape of the filter as fxf (≈1 line)\n",
    "                        shape = (f, f)\n",
    "                        # Distribute it to get the correct slice of dA_prev. i.e. Add the distributed value of da. (≈1 line)\n",
    "                        dA_prev[i, vert_start:vert_end, horiz_start:horiz_end, c] += distribute_value(da, shape)\n",
    "                            \n",
    "    return dA_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ryan_update_params(params, grads, learning_rate):\n",
    "    for i in range(int(len(params)//2)):\n",
    "        params[\"W\" + str(i + 1)] = params[\"W\" + str(i + 1)] - learning_rate * grads[\"dW\" + str(i + 1)]\n",
    "        params[\"b\" + str(i + 1)] = params[\"b\" + str(i + 1)] - learning_rate * grads[\"db\" + str(i + 1)]\n",
    "        \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A, con_caches, full_caches = ryan_forward_all(X_train, params)\n",
    "cost = ryan_compute_cost(A, Y_onehot.T)\n",
    "print(cost)\n",
    "grads = ryan_backward_all(A, Y_onehot.T, full_caches, con_caches)\n",
    "\n",
    "print(grads['dA1'].shape)\n",
    "print(grads['dW1'].shape)\n",
    "print(grads['db1'].shape)\n",
    "\n",
    "print(params['W1'].shape)\n",
    "print(params['b1'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ryan_update_params(params, grads, learning_rate):\n",
    "    for i in range(int(len(params)//2)):\n",
    "        params[\"W\" + str(i + 1)] = params[\"W\" + str(i + 1)] - learning_rate * grads[\"dW\" + str(i + 1)]\n",
    "        params[\"b\" + str(i + 1)] = params[\"b\" + str(i + 1)] - learning_rate * grads[\"db\" + str(i + 1)]\n",
    "            \n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "params = ryan_update_params(params, grads, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/huynh/anaconda3/envs/python3-env/lib/python3.6/site-packages/ipykernel_launcher.py:82: RuntimeWarning: overflow encountered in exp\n",
      "/Users/huynh/anaconda3/envs/python3-env/lib/python3.6/site-packages/ipykernel_launcher.py:64: RuntimeWarning: divide by zero encountered in log\n",
      "/Users/huynh/anaconda3/envs/python3-env/lib/python3.6/site-packages/ipykernel_launcher.py:64: RuntimeWarning: invalid value encountered in multiply\n"
     ]
    }
   ],
   "source": [
    "A, con_caches, full_caches = ryan_forward_all(X_train, params)\n",
    "cost = ryan_compute_cost(A, Y_onehot.T)\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
